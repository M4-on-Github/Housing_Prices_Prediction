import streamlit as st
import pandas as pd
import numpy as np
import joblib
from scipy.stats import skew
import os

# Set page config
st.set_page_config(page_title="House Price Predictor", page_icon="ðŸ ", layout="wide")

st.title("ðŸ  House Price Predictor")
st.markdown("Enter the details of the house to estimate its sale price.")

@st.cache_resource
def load_artifacts():
    model = joblib.load('static/model.joblib')
    scaler = joblib.load('static/scaler.joblib')
    feature_columns = joblib.load('static/feature_columns.joblib')
    reference_row = joblib.load('static/reference_row.joblib')
    categorical_values = joblib.load('static/categorical_values.joblib')
    return model, scaler, feature_columns, reference_row, categorical_values

# --- SIDEBAR ---
st.sidebar.header("ðŸ† Model Performance")
col_perf1, col_perf2 = st.sidebar.columns(2)
col_perf1.metric("RMSLE", "0.115")
st.sidebar.info("ðŸŽ¯ This score ranks in the **Top 2% to 5%** of the Kaggle leaderboard.")
st.sidebar.divider()

# --- PROJECT OVERVIEW ---
st.sidebar.header("ðŸ“‹ Project Overview")
st.sidebar.markdown(
    """
    This predictor is built using the **Ames Housing Dataset** from the [Kaggle House Prices Competition](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques).
    
    ### ðŸ§¬ The Model: Stacking Ensemble
    Instead of a single algorithm, we use a **Stacking Regressor** that aggregates four diverse models:
    1.  **XGBoost:** Excellent at capturing non-linear patterns and feature interactions.
    2.  **Gradient Boosting:** Focuses on minimizing residuals from previous trees.
    3.  **Ridge Regression:** A linear approach that uses L2 regularization to prevent overfitting.
    4.  **Lasso Regression:** Uses L1 regularization for inherent feature selection.
    
    **Final Meta-Model:** A RidgeCV regressor aggregates the predictions from these base models to produce the final estimate.
    
    ### ðŸ› ï¸ Preprocessing Pipeline
    To achieve high accuracy, the following steps were taken:
    - **Outlier Removal:** Removed extreme living area entries that didn't follow the price trend.
    - **Log Transformation:** Applied `log(1+x)` to the target price and skewed features to normalize distributions.
    - **Feature Engineering:** Created composite features like *Total Square Footage* and *Total Bathrooms*.
    - **Ordinal Encoding:** Converted quality ratings (Ex, Gd, TA, etc.) into numeric scales.
    - **Robust Scaling:** Standardized data while being resilient to remaining outliers.
    """
)

st.sidebar.divider()
st.sidebar.subheader("ðŸ’» Technical Stack")
st.sidebar.markdown(
    """
    - **Language:** Python 3.x
    - **ML Frameworks:** Scikit-Learn, XGBoost
    - **Data Ops:** Pandas, NumPy, SciPy
    - **UI Framework:** Streamlit
    - **Deployment:** Stlite (WebAssembly)
    - **Hosting:** GitHub Pages
    """
)

st.sidebar.divider()
st.sidebar.subheader("ðŸ”§ System Info")

try:
    import sklearn
    import xgboost
    import scipy
    
    st.sidebar.write(f"**SKLearn:** `{sklearn.__version__}`")
    st.sidebar.write(f"**XGBoost:** `{xgboost.__version__}`")
    
    # Polyfill for scikit-learn version mismatch (_loss module)
    import sys
    try:
        import sklearn._loss
    except ImportError:
        try:
            import sklearn.ensemble._gb_losses as losses
            sys.modules['sklearn._loss'] = losses
        except:
            pass

    model, scaler, feature_columns, reference_row, categorical_values = load_artifacts()
except Exception as e:
    st.error(f"### Error loading model artifacts")
    st.code(str(e))
    st.info("This often happens if the version of scikit-learn or xgboost in the browser doesn't match the version used to train the model.")
    st.stop()

# --- UI INPUTS ---
st.markdown("## ðŸ“Š Kaggle Dataset Features")
st.info("These interactive fields are mapped directly to the original [Kaggle Competition](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data) dataset.")

st.markdown("### ðŸŽ›ï¸ Configure Property Attributes")
st.write("Adjust the parameters below to see how property characteristics influence the predicted market value.")

col1, col2, col3 = st.columns(3)

with col1:
    st.subheader("ðŸ—ï¸ Structure & Quality")
    overall_qual = st.slider("Overall Quality", 1, 10, 6, 
                            help="Rates the overall material and finish of the house (1=Very Poor, 10=Very Excellent)")
    gr_liv_area = st.number_input("Living Area (sqft)", 300, 6000, 1500, step=50,
                                 help="Above grade (ground) living area square feet")
    total_bsmt_sf = st.number_input("Basement Area (sqft)", 0, 4000, 1000, step=50,
                                   help="Total square feet of basement area")
    year_built = st.number_input("Year Built", 1870, 2025, 2000, step=1,
                                help="Original construction date")

with col2:
    st.subheader("ðŸ›Œ Rooms & Space")
    full_bath = st.number_input("Full Bathrooms", 0, 5, 2, step=1,
                               help="Full bathrooms above grade")
    half_bath = st.number_input("Half Bathrooms", 0, 3, 0, step=1,
                               help="Half baths above grade")
    tot_rms_abv_grd = st.number_input("Total Rooms", 2, 15, 6, step=1,
                                     help="Total rooms above grade (does not include bathrooms)")
    garage_cars = st.number_input("Garage Capacity (Cars)", 0, 5, 2, step=1,
                                 help="Size of garage in car capacity")
    garage_area = st.number_input("Garage Area (sqft)", 0, 1500, 500, step=50,
                                 help="Size of garage in square feet")

with col3:
    st.subheader("ðŸ“ Location & Finish")
    # Dynamically get neighborhoods
    neighborhoods = sorted(categorical_values.get('Neighborhood', ['NAmes', 'CollgCr', 'OldTown', 'Edwards', 'Somerst']))
    neighborhood = st.selectbox("Neighborhood", neighborhoods, 
                                help="Physical locations within Ames city limits")
    
    exter_qual = st.selectbox("Exterior Quality", ['Ex', 'Gd', 'TA', 'Fa', 'Po'], index=2,
                             help="Evaluates the quality of the material on the exterior (Ex=Excellent, TA=Average/Typical)")
    kitchen_qual = st.selectbox("Kitchen Quality", ['Ex', 'Gd', 'TA', 'Fa', 'Po'], index=2,
                               help="Kitchen quality (Ex=Excellent, TA=Average/Typical)")
    fireplace_qu = st.selectbox("Fireplace Quality", ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'None'], index=5,
                               help="Fireplace quality (None if no fireplace)")

if st.button("ðŸ’° Predict Sale Price", use_container_width=True):
    # 1. Create a row with user inputs
    input_row = reference_row.copy()
    
    # Update with UI values
    input_row['OverallQual'] = overall_qual
    input_row['GrLivArea'] = gr_liv_area
    input_row['TotalBsmtSF'] = total_bsmt_sf
    input_row['YearBuilt'] = year_built
    input_row['FullBath'] = full_bath
    input_row['HalfBath'] = half_bath
    input_row['TotRmsAbvGrd'] = tot_rms_abv_grd
    input_row['GarageCars'] = garage_cars
    input_row['GarageArea'] = garage_area
    input_row['Neighborhood'] = neighborhood
    input_row['ExterQual'] = exter_qual
    input_row['KitchenQual'] = kitchen_qual
    input_row['FireplaceQu'] = fireplace_qu
    
    # 2. Preprocessing (matching notebook logic)
    # Note: We use a simplified version for a single row prediction
    
    # Feature Engineering
    input_row['TotalSF'] = input_row['TotalBsmtSF'] + input_row['1stFlrSF'] + input_row['2ndFlrSF']
    input_row['TotalBath'] = (input_row['FullBath'] + (0.5 * input_row['HalfBath']) + 
                             input_row['BsmtFullBath'] + (0.5 * input_row['BsmtHalfBath']))
    input_row['HasPool'] = input_row['PoolArea'].apply(lambda x: 1 if x > 0 else 0)
    input_row['Has2ndFloor'] = input_row['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)
    input_row['HasGarage'] = input_row['GarageArea'].apply(lambda x: 1 if x > 0 else 0)
    
    # Ordinal Encoding
    ordinal_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0}
    ordinal_cols = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 
                    'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']
    for col in ordinal_cols:
        if col in input_row.columns:
            input_row[col] = input_row[col].map(ordinal_map).fillna(0)

    # Convert object types to strings (to match training)
    for col in input_row.columns:
        if input_row[col].dtype == 'object':
            input_row[col] = input_row[col].astype(str)

    # Skew Correction (apply log1p to the same columns)
    # In a real app we'd save the list of skewed columns, but we can reuse the logic
    # Here we just apply it to the numerical ones we know are skewed or often skewed
    num_cols = input_row.select_dtypes(include=[np.number]).columns
    input_row[num_cols] = np.log1p(input_row[num_cols])

    # One-Hot Encoding
    input_row_encoded = pd.get_dummies(input_row)
    
    # Align columns with feature_columns from training
    final_input = pd.DataFrame(0, index=[0], columns=feature_columns)
    for col in input_row_encoded.columns:
        if col in final_input.columns:
            final_input[col] = input_row_encoded[col].iloc[0]
            
    # Scale
    final_input_scaled = scaler.transform(final_input)
    
    # Predict
    pred_log = model.predict(final_input_scaled)
    prediction = np.expm1(pred_log)[0]
    
    st.success(f"### Predicted Sale Price: ${prediction:,.2f}")
    
    st.info("Estimated based on your inputs and the Stacking Ensemble model.")

st.markdown("---")
st.caption("This model was trained on the Kaggle Housing Prices competition dataset.")
